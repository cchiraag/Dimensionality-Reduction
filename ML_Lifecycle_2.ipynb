{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Lifecycle - 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPxf0pLfyUS0EwFNEtClLpl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cchiraag/ML-Projects/blob/main/ML_Lifecycle_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0ZW9LsCoiyo"
      },
      "source": [
        "# **Spot-Check Classification Algorithms**\n",
        "It is the process of checking which algorithm is best suited for our ML problem. We cannot find which algorithm is appropriate for our problem beforehand. We have to spot-check each algorithm.\n",
        "\n",
        "In this chapter, we will perform spot-check on classification algorithms:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBQ32U1ZE1p8"
      },
      "source": [
        "## Logistic Regression\n",
        "It assumes the gaussian distribution for the input numeric variables. It can be used to perform binary classification problems.\n",
        "\n",
        "The logistic regression uses maximum iteration of 100, by default. To change it, we will use $max\\_iter$ argument, so that we would not give error to REACHED TO LIMIT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeLzh2hTnErI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3112a748-cba6-4307-c911-547bfabd560b"
      },
      "source": [
        "# Logistic Regression Classification\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "num_folds = 10\n",
        "kfold = KFold(n_splits=10)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7760423786739576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXF-OYscE8x0"
      },
      "source": [
        "## Linear Discriminant Analysis\n",
        "It also assumes the gaussian distribution of input variables, but can be used for binary and multiclass classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQMX7hptlL8E",
        "outputId": "2a251b47-1f60-499b-9c8b-34e58ac2c12f"
      },
      "source": [
        "# LDA Classification\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "num_folds = 10\n",
        "kfold = KFold(n_splits=10)\n",
        "model = LinearDiscriminantAnalysis()\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.773462064251538\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AiGdUpcFJwb"
      },
      "source": [
        "## k-Nearest Neighbours\n",
        "The KNN algorithm finds the k-nearest or similar neighbours in the training dataset using the distance metric, for the new variable. And takes the mean of these values to get the prediction. Here, the **Minkowski distance** is used, which is the generalized form of Euclidean (used when all inputs have the same scale) and Manhattan distance (for when the scales of the input variables differ)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6xo8fHTn4FV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbbcad6d-591f-40a4-a572-7ab082e08a78"
      },
      "source": [
        "# KNN Classification\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "num_folds = 10\n",
        "kfold = KFold(n_splits=10)\n",
        "model = KNeighborsClassifier()\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7265550239234451\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M38UJv0FQtg"
      },
      "source": [
        "## Naive Bayes\n",
        "It calculates the probability and conditional probability of each class wrt the given input variable. Then these probabilities are used to predict the probability of the new data and multiplied together to get the prediction (variables are assumed to be independent)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_M8aQReoLUD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68d0b7a6-bd5d-4061-cefc-0556213dec6c"
      },
      "source": [
        "# Gaussian Naive Bayes Classification\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "kfold = KFold(n_splits=10)\n",
        "model = GaussianNB()\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7551777170198223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kogyOmlAFXCs"
      },
      "source": [
        "## Classification and Regression Trees\n",
        "It constructs binary tree from the training dataset. Here, split points of each attributes are choosen wisely to get minimum cost. By default, the cost metric is mean squared error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BDZ_oRUoWHQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44048b94-2782-4b1e-f890-62a618e42d36"
      },
      "source": [
        "# CART Classification\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "kfold = KFold(n_splits=10)\n",
        "model = DecisionTreeClassifier()\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6874401913875599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3uxGBl5FhTq"
      },
      "source": [
        "## Support Vector Machines\n",
        "It is best line which separates the two given classes. A powerful Radial Basis Function is used, by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1Z5bOTyocrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d65e585-41f8-4711-caa9-0d2ac75d8e6f"
      },
      "source": [
        "# SVM Classification\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "kfold = KFold(n_splits=10)\n",
        "model = SVC()\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7604237867395763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo4nDoPZpCqD"
      },
      "source": [
        "# **Spot-Check Regression Algorithms**\n",
        "In this chapter, we will perform spot-check on regression algorithms:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3zJOHBXIX6s"
      },
      "source": [
        "## Linear Regression\n",
        "It is assumed that the input variable have gaussian distribution, and the attribute are not highly correlated to each other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSxvyWr5pFeX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55b317cf-3ff2-4e68-b15a-79ac755d5c64"
      },
      "source": [
        "# Linear Regression\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "filename = 'housing.csv'\n",
        "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
        "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:13]\n",
        "Y = array[:,13]\n",
        "kfold = KFold(n_splits=10)\n",
        "model = LinearRegression()\n",
        "scoring = 'neg_mean_squared_error'\n",
        "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-34.70525594452488\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIHXI0E3I18J"
      },
      "source": [
        "## Ridge Regression\n",
        "It is the extension of the linear regression, which modifies the loss function to minimize the complexity of the model, which is measured as L2-norm (i.e. the sum squared value of the coefficient values)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjQbE5u2qbGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e74c9ce-ed6c-40e7-e61e-b91f9011fb1a"
      },
      "source": [
        "# Ridge Regression\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import Ridge\n",
        "filename = 'housing.csv'\n",
        "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
        "'B', 'LSTAT', 'MEDV']\n",
        "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:13]\n",
        "Y = array[:,13]\n",
        "num_folds = 10\n",
        "kfold = KFold(n_splits=10)\n",
        "model = Ridge()\n",
        "scoring = 'neg_mean_squared_error'\n",
        "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-34.07824620925939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eILaxe46JSwp"
      },
      "source": [
        "## LASSO Regression\n",
        "The Least Absolute Shrinkage and Selection Operator is just like ridge, which is used to minimize the complexity of model, but measured as L1-norm (i.e. the sum absolute value of the coefficient values)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf1f4Ft4qmP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b04ac86d-c15b-4986-f10c-5f844feb17c2"
      },
      "source": [
        "# Lasso Regression\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import Lasso\n",
        "filename = 'housing.csv'\n",
        "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
        "'B', 'LSTAT', 'MEDV']\n",
        "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:13]\n",
        "Y = array[:,13]\n",
        "kfold = KFold(n_splits=10)\n",
        "model = Lasso()\n",
        "scoring = 'neg_mean_squared_error'\n",
        "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-34.46408458830232\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZPlC3T2KG8d"
      },
      "source": [
        "## ElasticNet Regression\n",
        "It is the form of regularization regression which combines the properties of Ridge and LASSO (i.e. it minimizes the complexity using L1 and L2-norm)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDSQ_pzoqyxX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25709fd5-8fca-4128-842b-00c2412c0f34"
      },
      "source": [
        "# ElasticNet Regression\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import ElasticNet\n",
        "filename = 'housing.csv'\n",
        "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
        "'B', 'LSTAT', 'MEDV']\n",
        "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:13]\n",
        "Y = array[:,13]\n",
        "kfold = KFold(n_splits=10)\n",
        "model = ElasticNet()\n",
        "scoring = 'neg_mean_squared_error'\n",
        "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-31.164573714249762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH9418UFVGLm"
      },
      "source": [
        "## k-Nearest Neighbours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n71FzOc8rLUx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aeaede2-52de-425a-bf32-a3e4795f3379"
      },
      "source": [
        "# KNN Regression\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "filename = 'housing.csv'\n",
        "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
        "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:13]\n",
        "Y = array[:,13]\n",
        "kfold = KFold(n_splits=10)\n",
        "model = KNeighborsRegressor()\n",
        "scoring = 'neg_mean_squared_error'\n",
        "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-107.28683898039215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhaTpkA3VMmN"
      },
      "source": [
        "## Classification and Regression Trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUa76RY1rNMQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aad378a-9cb2-4a08-a29e-6125b5630ab1"
      },
      "source": [
        "# Decision Tree Regression\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "filename = 'housing.csv'\n",
        "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
        "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:13]\n",
        "Y = array[:,13]\n",
        "kfold = KFold(n_splits=10)\n",
        "model = DecisionTreeRegressor()\n",
        "scoring = 'neg_mean_squared_error'\n",
        "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-39.503545490196075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD8rtLMSVTAe"
      },
      "source": [
        "## Support Vector Machines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tdnp40LsrT3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "998adee5-b008-450f-9f69-8c4a00fb088d"
      },
      "source": [
        "# SVM Regression\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import SVR\n",
        "filename = 'housing.csv'\n",
        "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
        "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:13]\n",
        "Y = array[:,13]\n",
        "num_folds = 10\n",
        "kfold = KFold(n_splits=10)\n",
        "model = SVR()\n",
        "scoring = 'neg_mean_squared_error'\n",
        "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-72.25543311855311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSaIUMKQrd2j"
      },
      "source": [
        "# **Compare Machine Learning Algorithms**\n",
        "In this section, we will create a **test harness** to compare different ML algorithms. We can even use this test harness as a template, for our further ML problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3P87ZbGrf62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "7819cc54-de06-4d85-ac3e-7e41df9e7888"
      },
      "source": [
        "# Compare Algorithms\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "# load dataset\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "# prepare models\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression(max_iter=1000)))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('CART', DecisionTreeClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC()))\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in models:\n",
        "  kfold = KFold(n_splits=10)\n",
        "  cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR: 0.776042 (0.051575)\n",
            "LDA: 0.773462 (0.051592)\n",
            "KNN: 0.726555 (0.061821)\n",
            "CART: 0.699129 (0.057542)\n",
            "NB: 0.755178 (0.042766)\n",
            "SVM: 0.760424 (0.052931)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcOKjUWiW0Yp"
      },
      "source": [
        "Through the above codes and its coressponding output, we have calculated the accuracy of different algorithms in our model.\n",
        "\n",
        "Thus, from the output we got that **Logistic Regression** and **LDA** are best-suited algorithm for this ML problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr51libcsoMv"
      },
      "source": [
        "# **Automate Machine Learning Workflows with Pipelines**\n",
        "There are certain standard workflows in ML which can automated, which is achieved through **Pipelines**. So, in this section we will these Pipelines to automate our ML workflows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5cboS_jYXV2"
      },
      "source": [
        "## Data Preparation and Model Pipeline\n",
        "For the process of Data Preparation, the **Pipelines** helps to prevent data leakage in the test harness by ensuring that data preparation like standardization is constrained to each fold of our cross-validation procedure.\n",
        "\n",
        "The given pipeline is defined with two steps:\n",
        "1. Standardize the data.\n",
        "2. Learn an LDA model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-Df-poKsrvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f75a8a11-2d58-455b-bd57-d97d7ec8fcbc"
      },
      "source": [
        "# Create a pipeline that standardizes the data then creates a model\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "# load data\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "# create pipeline\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('lda', LinearDiscriminantAnalysis()))\n",
        "model = Pipeline(estimators)\n",
        "# evaluate pipeline\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.773462064251538\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_6ZtpdHahzY"
      },
      "source": [
        "## Feature Extraction and Modelling Pipeline\n",
        "The **Pipelines** consists of a tool called $FeatureUnion$ which allows the results of multiple feature selection and extraction procedures to be combined into a larger dataset on which a model can be trained. Importantly, all the feature extraction and the feature union occurs within each fold of the cross validation procedure. \n",
        "\n",
        "The pipeline for the given example consists of:\n",
        "1. Feature Extraction with Principal Component Analysis (3 features).\n",
        "2. Feature Extraction with Statistical Selection (6 features).\n",
        "3. Feature Union.\n",
        "4. Learn a Logistic Regression Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqnbcNchs5xS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1da29442-4f4e-4e55-861f-ac3b73964c8b"
      },
      "source": [
        "# Create a pipeline that extracts features from the data then creates a model\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "# load data\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "# create feature union\n",
        "features = []\n",
        "features.append(('pca', PCA(n_components=3)))\n",
        "features.append(('select_best', SelectKBest(k=6)))\n",
        "feature_union = FeatureUnion(features)\n",
        "# create pipeline\n",
        "estimators = []\n",
        "estimators.append(('feature_union', feature_union))\n",
        "estimators.append(('logistic', LogisticRegression(max_iter=1000)))\n",
        "model = Pipeline(estimators)\n",
        "# evaluate pipeline\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7760423786739576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPLBMgwbcK8v"
      },
      "source": [
        "# **Improve Performance with Ensembles**\n",
        "In this section, we will learn how to use **ensembles** to boost up the accuracy of our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vElG7OVdcrtx"
      },
      "source": [
        "## Bagging Algorithms\n",
        "Bagging/Bootstrapping algorithms takes multiple samples from training dataset with replacement, and the average of the coressponding output is our predicted output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEt5f_rTdpTf"
      },
      "source": [
        "### Bagged Decision Trees\n",
        "Bagging perform best with the algorithms having high variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6zB6R0gtO6j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dff18fa7-f1a7-4715-8742-dc972856df37"
      },
      "source": [
        "# Bagged Decision Trees for Classification\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "seed = 7\n",
        "kfold = KFold(n_splits=10)\n",
        "cart = DecisionTreeClassifier()\n",
        "num_trees = 100\n",
        "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.770745044429255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJCg9witd9BQ"
      },
      "source": [
        "### Random Forest\n",
        "It is extension of Bagged Decision Trees, in which the samples are taken with replacement, but the tree is constructed in such a way that reduces the correlation among different other classifiers. For construction of trees, it doesnot chooses the best split of features, instead it chooses the split point randomly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePOyFb35tYKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d701340b-e40b-4d98-eaac-dead90bdf67d"
      },
      "source": [
        "# Random Forest Classification\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "num_trees = 100\n",
        "max_features = 3\n",
        "kfold = KFold(n_splits=10)\n",
        "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7720266575529734\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-Uh8jWLe3Tp"
      },
      "source": [
        "### Extra Trees\n",
        "Here, the random trees are constructed from the random samples of training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfJec29-trlS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8181672-ef44-429a-d442-1a41db1989f9"
      },
      "source": [
        "# Extra Trees Classification\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "num_trees = 100\n",
        "max_features = 7\n",
        "kfold = KFold(n_splits=10)\n",
        "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7642857142857142\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh8mwBldfuBh"
      },
      "source": [
        "## Boosting Algorithms\n",
        "It attempts to correct the mistakes of a model by creating a sequence of models. On the basis of the accuracy and performance of these models, the combined final prediction is made."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNbzbfASg6Hb"
      },
      "source": [
        "### AdaBoost\n",
        "This algorithm works by giving weightage to those instances which are difficult of being classified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QMCjNqcudth",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3093ea5b-c094-425c-c4f7-910c6f5d85e8"
      },
      "source": [
        "# AdaBoost Classification\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "num_trees = 30\n",
        "seed=7\n",
        "kfold = KFold(n_splits=10)\n",
        "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.760457963089542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSIAJtd6hvpU"
      },
      "source": [
        "### Stochastic Gradient Boosting\n",
        "It is one of the best ensemble method to improve the performance of the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnuQcxysuxF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc60ae6-0d22-405d-bad2-17f7afa8d844"
      },
      "source": [
        "# Stochastic Gradient Boosting Classification\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "seed = 7\n",
        "num_trees = 100\n",
        "kfold = KFold(n_splits=10)\n",
        "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7681989063568012\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogL__iWQiLiG"
      },
      "source": [
        "## Voting Ensemble\n",
        "This algorithm is used to give the averaged final output of the predictions made by several sub-models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Alg_CGO9BZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4c2aac0-398c-4e9e-90db-09f179b50e0d"
      },
      "source": [
        "# Voting Ensemble for Classification\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "kfold = KFold(n_splits=10)\n",
        "# create the sub models\n",
        "estimators = []\n",
        "model1 = LogisticRegression(max_iter=1000)\n",
        "estimators.append(('logistic', model1))\n",
        "model2 = DecisionTreeClassifier()\n",
        "estimators.append(('cart', model2))\n",
        "model3 = SVC()\n",
        "estimators.append(('svm', model3))\n",
        "# create the ensemble model\n",
        "ensemble = VotingClassifier(estimators)\n",
        "results = cross_val_score(ensemble, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7669343814080657\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urhFVoA89gIn"
      },
      "source": [
        "# **Improve Performance with Algorithm Tuning**\n",
        "In this section, we will learn to tune-up the performance of ML algorithms. Here, the ML algorithms are parameterized and we will use different search strategies to find the best parameters for our ML model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWRUA3EZkB-C"
      },
      "source": [
        "## Grid Search Parameter Tuning\n",
        "This method evaluates the combination for each one of the parameters, which is specified in a grid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJiO4W4r9lxL",
        "outputId": "2a76278a-9526-4282-f57f-00846482a2e4"
      },
      "source": [
        "# Grid Search for Algorithm Tuning\n",
        "import numpy\n",
        "from pandas import read_csv\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "alphas = numpy.array([1,0.1,0.01,0.001,0.0001,0])\n",
        "param_grid = dict(alpha=alphas)\n",
        "model = Ridge()\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
        "grid.fit(X, Y)\n",
        "print(grid.best_score_)\n",
        "print(grid.best_estimator_.alpha)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.27610844129292433\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC6EfVTrk6A3"
      },
      "source": [
        "## Random Search Parameter Tuning\n",
        "This method evaluates the random no. of parameters in a random distribution of data for a fixed no. of iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yba5d0yE9wBF",
        "outputId": "99bba6b9-6905-4b87-8fbd-de43d8b2d2e2"
      },
      "source": [
        "# Randomized for Algorithm Tuning\n",
        "import numpy\n",
        "from pandas import read_csv\n",
        "from scipy.stats import uniform\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "param_grid = {'alpha': uniform()}\n",
        "model = Ridge()\n",
        "rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100,random_state=7)\n",
        "rsearch.fit(X, Y)\n",
        "print(rsearch.best_score_)\n",
        "print(rsearch.best_estimator_.alpha)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.27610755734028525\n",
            "0.9779895119966027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGW9ANHc99Mc"
      },
      "source": [
        "# **Save and Load Machine Learning Models**\n",
        "In this final section, we will learn how to save and load our ML models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ammVkwMxln23"
      },
      "source": [
        "## Finalize your model using Pickle\n",
        "Pickle is the standard way to serialize our ML model, and save the serialized model to our drive. Later, we can load it to deserialize and make predictions or changes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGOU3LCl-BGH",
        "outputId": "eb91ab00-9ebd-4c86-8f7a-cbfba6329bcd"
      },
      "source": [
        "# Save Model Using Pickle\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from pickle import dump\n",
        "from pickle import load\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
        "# Fit the model on 33%\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, Y_train)\n",
        "# save the model to disk\n",
        "filename = 'finalized_model.sav'\n",
        "dump(model, open(filename, 'wb'))\n",
        "# some time later...\n",
        "# load the model from disk\n",
        "loaded_model = load(open(filename, 'rb'))\n",
        "result = loaded_model.score(X_test, Y_test)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7874015748031497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tva6-RhRmYQH"
      },
      "source": [
        "## Finalize your model using Joblib\n",
        "Joblib is used for providing SciPy ecosystem and utilities for pipelining. It is also used to save and load our ML model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97vRlWii-OXg",
        "outputId": "5ca3b788-0087-4535-bcd4-11add4c56bf4"
      },
      "source": [
        "# Save Model Using joblib\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.externals.joblib import dump\n",
        "from sklearn.externals.joblib import load\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
        "# Fit the model on 33%\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, Y_train)\n",
        "# save the model to disk\n",
        "filename = 'finalized_model.sav'\n",
        "dump(model, filename)\n",
        "# some time later...\n",
        "# load the model from disk\n",
        "loaded_model = load(filename)\n",
        "result = loaded_model.score(X_test, Y_test)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7874015748031497\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}